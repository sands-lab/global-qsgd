{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([-100.,  -99.,  -98.,  -97.,  -96.,  -95.,  -94.,  -93.,  -92.,  -91.,\n",
      "         -90.,  -89.,  -88.,  -87.,  -86.,  -85.,  -84.,  -83.,  -82.,  -81.,\n",
      "         -80.,  -79.,  -78.,  -77.,  -76.,  -75.,  -74.,  -73.,  -72.,  -71.,\n",
      "         -70.,  -69.,  -68.,  -67.,  -66.,  -65.,  -64.,  -63.,  -62.,  -61.,\n",
      "         -60.,  -59.,  -58.,  -57.,  -56.,  -55.,  -54.,  -53.,  -52.,  -51.,\n",
      "         -50.,  -49.,  -48.,  -47.,  -46.,  -45.,  -44.,  -43.,  -42.,  -41.,\n",
      "         -40.,  -39.,  -38.,  -37.,  -36.,  -35.,  -34.,  -33.,  -32.,  -31.,\n",
      "         -30.,  -29.,  -28.,  -27.,  -26.,  -25.,  -24.,  -23.,  -22.,  -21.,\n",
      "         -20.,  -19.,  -18.,  -17.,  -16.,  -15.,  -14.,  -13.,  -12.,  -11.,\n",
      "         -10.,   -9.,   -8.,   -7.,   -6.,   -5.,   -4.,   -3.,   -2.,   -1.,\n",
      "           0.,    1.,    2.,    3.,    4.,    5.,    6.,    7.,    8.,    9.,\n",
      "          10.,   11.,   12.,   13.,   14.,   15.,   16.,   17.,   18.,   19.,\n",
      "          20.,   21.,   22.,   23.,   24.,   25.,   26.,   27.,   28.,   29.,\n",
      "          30.,   31.,   32.,   33.,   34.,   35.,   36.,   37.,   38.,   39.,\n",
      "          40.,   41.,   42.,   43.,   44.,   45.,   46.,   47.,   48.,   49.,\n",
      "          50.,   51.,   52.,   53.,   54.,   55.,   56.,   57.,   58.,   59.,\n",
      "          60.,   61.,   62.,   63.,   64.,   65.,   66.,   67.,   68.,   69.,\n",
      "          70.,   71.,   72.,   73.,   74.,   75.,   76.,   77.,   78.,   79.,\n",
      "          80.,   81.,   82.,   83.,   84.,   85.,   86.,   87.,   88.,   89.,\n",
      "          90.,   91.,   92.,   93.,   94.,   95.,   96.,   97.,   98.,   99.])\n",
      "global_min: tensor(-100.)\n",
      "global_max: tensor(99.)\n",
      "normalized: tensor([0.0000, 0.0050, 0.0101, 0.0151, 0.0201, 0.0251, 0.0302, 0.0352, 0.0402,\n",
      "        0.0452, 0.0503, 0.0553, 0.0603, 0.0653, 0.0704, 0.0754, 0.0804, 0.0854,\n",
      "        0.0905, 0.0955, 0.1005, 0.1055, 0.1106, 0.1156, 0.1206, 0.1256, 0.1307,\n",
      "        0.1357, 0.1407, 0.1457, 0.1508, 0.1558, 0.1608, 0.1658, 0.1709, 0.1759,\n",
      "        0.1809, 0.1859, 0.1910, 0.1960, 0.2010, 0.2060, 0.2111, 0.2161, 0.2211,\n",
      "        0.2261, 0.2312, 0.2362, 0.2412, 0.2462, 0.2513, 0.2563, 0.2613, 0.2663,\n",
      "        0.2714, 0.2764, 0.2814, 0.2864, 0.2915, 0.2965, 0.3015, 0.3065, 0.3116,\n",
      "        0.3166, 0.3216, 0.3266, 0.3317, 0.3367, 0.3417, 0.3467, 0.3518, 0.3568,\n",
      "        0.3618, 0.3668, 0.3719, 0.3769, 0.3819, 0.3869, 0.3920, 0.3970, 0.4020,\n",
      "        0.4070, 0.4121, 0.4171, 0.4221, 0.4271, 0.4322, 0.4372, 0.4422, 0.4472,\n",
      "        0.4523, 0.4573, 0.4623, 0.4673, 0.4724, 0.4774, 0.4824, 0.4874, 0.4925,\n",
      "        0.4975, 0.5025, 0.5075, 0.5126, 0.5176, 0.5226, 0.5276, 0.5327, 0.5377,\n",
      "        0.5427, 0.5477, 0.5528, 0.5578, 0.5628, 0.5678, 0.5729, 0.5779, 0.5829,\n",
      "        0.5879, 0.5930, 0.5980, 0.6030, 0.6080, 0.6131, 0.6181, 0.6231, 0.6281,\n",
      "        0.6332, 0.6382, 0.6432, 0.6482, 0.6533, 0.6583, 0.6633, 0.6683, 0.6734,\n",
      "        0.6784, 0.6834, 0.6884, 0.6935, 0.6985, 0.7035, 0.7085, 0.7136, 0.7186,\n",
      "        0.7236, 0.7286, 0.7337, 0.7387, 0.7437, 0.7487, 0.7538, 0.7588, 0.7638,\n",
      "        0.7688, 0.7739, 0.7789, 0.7839, 0.7889, 0.7940, 0.7990, 0.8040, 0.8090,\n",
      "        0.8141, 0.8191, 0.8241, 0.8291, 0.8342, 0.8392, 0.8442, 0.8492, 0.8543,\n",
      "        0.8593, 0.8643, 0.8693, 0.8744, 0.8794, 0.8844, 0.8894, 0.8945, 0.8995,\n",
      "        0.9045, 0.9095, 0.9146, 0.9196, 0.9246, 0.9296, 0.9347, 0.9397, 0.9447,\n",
      "        0.9497, 0.9548, 0.9598, 0.9648, 0.9698, 0.9749, 0.9799, 0.9849, 0.9899,\n",
      "        0.9950, 1.0000])\n",
      "scaled: tensor([0.0000, 0.0352, 0.0704, 0.1055, 0.1407, 0.1759, 0.2111, 0.2462, 0.2814,\n",
      "        0.3166, 0.3518, 0.3869, 0.4221, 0.4573, 0.4925, 0.5276, 0.5628, 0.5980,\n",
      "        0.6332, 0.6683, 0.7035, 0.7387, 0.7739, 0.8090, 0.8442, 0.8794, 0.9146,\n",
      "        0.9497, 0.9849, 1.0201, 1.0553, 1.0905, 1.1256, 1.1608, 1.1960, 1.2312,\n",
      "        1.2663, 1.3015, 1.3367, 1.3719, 1.4070, 1.4422, 1.4774, 1.5126, 1.5477,\n",
      "        1.5829, 1.6181, 1.6533, 1.6884, 1.7236, 1.7588, 1.7940, 1.8291, 1.8643,\n",
      "        1.8995, 1.9347, 1.9698, 2.0050, 2.0402, 2.0754, 2.1106, 2.1457, 2.1809,\n",
      "        2.2161, 2.2513, 2.2864, 2.3216, 2.3568, 2.3920, 2.4271, 2.4623, 2.4975,\n",
      "        2.5327, 2.5678, 2.6030, 2.6382, 2.6734, 2.7085, 2.7437, 2.7789, 2.8141,\n",
      "        2.8492, 2.8844, 2.9196, 2.9548, 2.9899, 3.0251, 3.0603, 3.0955, 3.1307,\n",
      "        3.1658, 3.2010, 3.2362, 3.2714, 3.3065, 3.3417, 3.3769, 3.4121, 3.4472,\n",
      "        3.4824, 3.5176, 3.5528, 3.5879, 3.6231, 3.6583, 3.6935, 3.7286, 3.7638,\n",
      "        3.7990, 3.8342, 3.8693, 3.9045, 3.9397, 3.9749, 4.0100, 4.0452, 4.0804,\n",
      "        4.1156, 4.1508, 4.1859, 4.2211, 4.2563, 4.2915, 4.3266, 4.3618, 4.3970,\n",
      "        4.4322, 4.4673, 4.5025, 4.5377, 4.5729, 4.6080, 4.6432, 4.6784, 4.7136,\n",
      "        4.7487, 4.7839, 4.8191, 4.8543, 4.8894, 4.9246, 4.9598, 4.9950, 5.0302,\n",
      "        5.0653, 5.1005, 5.1357, 5.1709, 5.2060, 5.2412, 5.2764, 5.3116, 5.3467,\n",
      "        5.3819, 5.4171, 5.4523, 5.4874, 5.5226, 5.5578, 5.5930, 5.6281, 5.6633,\n",
      "        5.6985, 5.7337, 5.7688, 5.8040, 5.8392, 5.8744, 5.9095, 5.9447, 5.9799,\n",
      "        6.0151, 6.0503, 6.0854, 6.1206, 6.1558, 6.1910, 6.2261, 6.2613, 6.2965,\n",
      "        6.3317, 6.3668, 6.4020, 6.4372, 6.4724, 6.5075, 6.5427, 6.5779, 6.6131,\n",
      "        6.6482, 6.6834, 6.7186, 6.7538, 6.7889, 6.8241, 6.8593, 6.8945, 6.9296,\n",
      "        6.9648, 7.0000])\n",
      "quantized: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 7], dtype=torch.uint8)\n",
      "packed: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], dtype=torch.uint8)\n",
      "packed: tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          1,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,\n",
      "         18,  34,  34,  34,  34,  34,  34,  34,  34,  34,  34,  34,  34,  34,\n",
      "         34,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,\n",
      "         51,  68,  68,  68,  68,  68,  68,  68,  68,  68,  68,  68,  68,  68,\n",
      "         68,  69,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,  85,\n",
      "         85,  86, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102,\n",
      "        102, 103], dtype=torch.uint8)\n",
      "packed.size(): torch.Size([100])\n",
      "quantized.size(): torch.Size([200])\n",
      "unpacked: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 7], dtype=torch.uint8)\n",
      "unpacked allclose to quantized: True\n",
      "unpacked_float: tensor([-100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
      "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
      "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
      "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
      "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,  -71.5714,\n",
      "         -71.5714,  -71.5714,  -71.5714,  -71.5714,  -71.5714,  -71.5714,\n",
      "         -71.5714,  -71.5714,  -71.5714,  -71.5714,  -71.5714,  -71.5714,\n",
      "         -71.5714,  -71.5714,  -71.5714,  -71.5714,  -71.5714,  -71.5714,\n",
      "         -71.5714,  -71.5714,  -71.5714,  -71.5714,  -71.5714,  -71.5714,\n",
      "         -71.5714,  -71.5714,  -71.5714,  -43.1429,  -43.1429,  -43.1429,\n",
      "         -43.1429,  -43.1429,  -43.1429,  -43.1429,  -43.1429,  -43.1429,\n",
      "         -43.1429,  -43.1429,  -43.1429,  -43.1429,  -43.1429,  -43.1429,\n",
      "         -43.1429,  -43.1429,  -43.1429,  -43.1429,  -43.1429,  -43.1429,\n",
      "         -43.1429,  -43.1429,  -43.1429,  -43.1429,  -43.1429,  -43.1429,\n",
      "         -43.1429,  -43.1429,  -14.7143,  -14.7143,  -14.7143,  -14.7143,\n",
      "         -14.7143,  -14.7143,  -14.7143,  -14.7143,  -14.7143,  -14.7143,\n",
      "         -14.7143,  -14.7143,  -14.7143,  -14.7143,  -14.7143,  -14.7143,\n",
      "         -14.7143,  -14.7143,  -14.7143,  -14.7143,  -14.7143,  -14.7143,\n",
      "         -14.7143,  -14.7143,  -14.7143,  -14.7143,  -14.7143,  -14.7143,\n",
      "          13.7143,   13.7143,   13.7143,   13.7143,   13.7143,   13.7143,\n",
      "          13.7143,   13.7143,   13.7143,   13.7143,   13.7143,   13.7143,\n",
      "          13.7143,   13.7143,   13.7143,   13.7143,   13.7143,   13.7143,\n",
      "          13.7143,   13.7143,   13.7143,   13.7143,   13.7143,   13.7143,\n",
      "          13.7143,   13.7143,   13.7143,   13.7143,   13.7143,   42.1429,\n",
      "          42.1429,   42.1429,   42.1429,   42.1429,   42.1429,   42.1429,\n",
      "          42.1429,   42.1429,   42.1429,   42.1429,   42.1429,   42.1429,\n",
      "          42.1429,   42.1429,   42.1429,   42.1429,   42.1429,   42.1429,\n",
      "          42.1429,   42.1429,   42.1429,   42.1429,   42.1429,   42.1429,\n",
      "          42.1429,   42.1429,   42.1429,   70.5714,   70.5714,   70.5714,\n",
      "          70.5714,   70.5714,   70.5714,   70.5714,   70.5714,   70.5714,\n",
      "          70.5714,   70.5714,   70.5714,   70.5714,   70.5714,   70.5714,\n",
      "          70.5714,   70.5714,   70.5714,   70.5714,   70.5714,   70.5714,\n",
      "          70.5714,   70.5714,   70.5714,   70.5714,   70.5714,   70.5714,\n",
      "          70.5714,   99.0000])\n"
     ]
    }
   ],
   "source": [
    "INTERVALS = 7\n",
    "# 创建一个从-100到100的浮点数张量作为输入数据\n",
    "input =torch.arange(-100,100,1, dtype=torch.float32)  # 注意：torch.range已弃用，应使用torch.arange\n",
    "print(\"input:\",input)\n",
    "\n",
    "# 获取输入张量的最小值和最大值，用于归一化\n",
    "global_min = input.min()\n",
    "global_max = input.max()\n",
    "print(\"global_min:\",global_min)\n",
    "print(\"global_max:\",global_max)\n",
    "\n",
    "# 将输入数据归一化到[0,1]范围内\n",
    "normalized = (input - global_min) / (global_max - global_min)\n",
    "print(\"normalized:\",normalized)\n",
    "\n",
    "# 将归一化后的数据缩放到[0,7]范围，以便进行4位量化（0-7范围可以用3位表示，但使用4位存储）\n",
    "scaled = normalized * INTERVALS\n",
    "print(\"scaled:\",scaled)\n",
    "\n",
    "# 将缩放后的数据裁剪到[0,7]范围并转换为无符号8位整数类型\n",
    "# 虽然转换为uint8，但实际值仅使用低3位(0-7)\n",
    "quantized = torch.clamp(scaled, 0, 7).to(torch.uint8)\n",
    "print(\"quantized:\",quantized)\n",
    "\n",
    "\n",
    "# 创建一个张量来存储压缩后的数据\n",
    "# 由于每个字节可以存储两个4位值，所以大小为原始张量的一半（向上取整）\n",
    "packed = torch.zeros((quantized.numel() + 1) // 2, dtype=torch.uint8, device=quantized.device)\n",
    "print(\"packed:\",packed)\n",
    "\n",
    "#### Compress 8bits -> 4 bits\n",
    "# 将量化后的4位值打包到8位字节中\n",
    "# 每个字节可以存储两个4位值：低4位存储奇数索引的值，高4位存储偶数索引的值\n",
    "# 压缩过程：\n",
    "#   1. 将偶数索引的值左移4位，放入高4位\n",
    "#   2. 将奇数索引的值直接放入低4位\n",
    "#   3. 使用按位或运算将两个值合并到一个字节中\n",
    "\n",
    "# 将偶数索引(0,2,4...)的量化值左移4位放入packed的高4位\n",
    "packed[:quantized[::2].numel()] = quantized[::2] << 4  \n",
    "\n",
    "# 如果有奇数个元素，确保处理奇数索引的值\n",
    "if quantized.numel() > 1:\n",
    "    # 使用按位或运算将奇数索引(1,3,5...)的量化值放入packed的低4位\n",
    "    packed[:quantized[1::2].numel()] |= quantized[1::2]\n",
    "\n",
    "print(\"packed:\",packed)\n",
    "print(\"packed.size():\",packed.size())  # 显示压缩后的张量大小\n",
    "print(\"quantized.size():\",quantized.size())  # 显示原始张量大小\n",
    "\n",
    "### Decompress 4 bits -> 8 bits\n",
    "# 解压缩过程：从压缩的字节中提取4位值并还原为原始量化值\n",
    "\n",
    "# 创建一个与原始量化张量相同大小的张量来存储解压缩后的数据\n",
    "unpacked = torch.zeros(quantized.numel(), dtype=torch.uint8, device=packed.device)\n",
    "\n",
    "# 计算需要处理的偶数位元素数量\n",
    "# 取packed.numel()和(unpacked.numel() // 2 + unpacked.numel() % 2)的较小值\n",
    "# 这确保我们不会超出packed或unpacked的边界\n",
    "num_even = min(packed.numel(), unpacked.numel() // 2 + unpacked.numel() % 2)\n",
    "\n",
    "# 从packed的高4位提取数据并还原到unpacked的偶数位置\n",
    "# 右移4位后与0x0F(00001111)进行按位与操作，确保只保留低4位\n",
    "unpacked[:2*num_even:2] = (packed[:num_even] >> 4) & 0x0F  \n",
    "\n",
    "# 计算需要处理的奇数位元素数量\n",
    "num_odd = min(packed.numel(), unpacked.numel() // 2)\n",
    "\n",
    "# 从packed的低4位提取数据并还原到unpacked的奇数位置\n",
    "# 与0x0F(00001111)进行按位与操作，确保只保留低4位\n",
    "unpacked[1:2*num_odd:2] = packed[:num_odd] & 0x0F          \n",
    "\n",
    "print(\"unpacked:\",unpacked)\n",
    "# 验证解压缩是否正确，比较解压缩后的张量与原始量化张量是否相等\n",
    "print(\"unpacked allclose to quantized:\", torch.allclose(unpacked.float(), quantized.float()))\n",
    "\n",
    "# 去归一化\n",
    "unpacked_float = unpacked.to(torch.float)/INTERVALS * (global_max - global_min) + global_min\n",
    "print(\"unpacked_float:\",unpacked_float)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
